{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Code Snippets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Remove HTML tags\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    import re\n",
    "    clean = re.compile('<.*?>')\n",
    "    return re.sub(clean, '', text)\n",
    "\n",
    "#Remove extra whitespaces\n",
    "def remove_extra_space(txt):\n",
    "    import re\n",
    "    return re.sub(' +',' ',txt)\n",
    "\n",
    "#Language identification \n",
    "def language(txt):\n",
    "    #pip install langdetect\n",
    "    import langdetect\n",
    "    lan = langdetect.detect(txt)\n",
    "    if lan == \"en\":\n",
    "        print(\"Uploaded Language is English\")\n",
    "        return (txt)\n",
    "    else:\n",
    "        return(\"Uploaded language is not in english\")\n",
    "\n",
    "#Remove non-ASCII characters from list of tokenized words   \n",
    "def remove_non_ascii(words):\n",
    "    import unicodedata\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "        rst = ''.join(new_words)\n",
    "    return rst    \n",
    "    \n",
    "#Expand contractions\n",
    "def expand_contraction(txt):\n",
    "    #pip install contractions\n",
    "    import contractions\n",
    "    return contractions.fix(txt)\n",
    "\n",
    "#Remove special characters\n",
    "def remove_punctuation(txt):\n",
    "    import string\n",
    "    result = txt.translate(str.maketrans('','',string.punctuation))\n",
    "    return result\n",
    "\n",
    "#Lowercase all texts\n",
    "def lower_text(txt):\n",
    "    return txt.lower()\n",
    "\n",
    "#Remove numbers\n",
    "def remove_no(txt):\n",
    "    import re\n",
    "    return re.sub(r\"\\d+\",\"\",txt)\n",
    "\n",
    "# convert number into words \n",
    "def convert_number(text): \n",
    "    #pip install inflect \n",
    "    # split string into list of words \n",
    "    temp_str = text.split() \n",
    "    # initialise empty list \n",
    "    new_string = [] \n",
    "    for word in temp_str: \n",
    "        # if word is a digit, convert the digit \n",
    "        # to numbers and append into the new_string list \n",
    "        if word.isdigit(): \n",
    "            temp = p.number_to_words(word) \n",
    "            new_string.append(temp) \n",
    "  \n",
    "        # append the word as it is \n",
    "        else: \n",
    "            new_string.append(word) \n",
    "  \n",
    "    # join the words of new_string to form a string \n",
    "    temp_str = ' '.join(new_string) \n",
    "    return temp_str \n",
    "\n",
    "#Text Normalization\n",
    "def normalize(txt):\n",
    "    words = remove_non_ascii(txt)\n",
    "    words = lower_text(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_no(words)\n",
    "    return words\n",
    "#Remove URL\n",
    "def removeurl(txt):\n",
    "    import re\n",
    "    return re.sub(r'http?\\S+|www\\.\\S+', '',txt)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "#named entity recognition\n",
    "def name_entity(text):\n",
    "    import spacy\n",
    "    sp = spacy.load('en_core_web_sm')\n",
    "    sen = sp(text)\n",
    "    txt = []\n",
    "    entity = []\n",
    "    for i in sen.ents:\n",
    "            txt.append(i.text)\n",
    "            entity.append(i.label_)\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame({\"Text\":txt,\n",
    "                    \"Entity\":entity})\n",
    "    return df\n",
    "\n",
    "def name_entity_list(text_list):\n",
    "    import spacy\n",
    "    sp = spacy.load('en_core_web_sm')\n",
    "    txt = []\n",
    "    entity = []\n",
    "    for i in text_list:\n",
    "        sen = sp(i)\n",
    "        for i in sen.ents:\n",
    "            txt.append(i.text)\n",
    "            entity.append(i.label_)\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame({\"Text\":txt,\n",
    "                      \"Entity\":entity})\n",
    "    return df\n",
    "\n",
    "\"/--------------------------------------Tokenization------------------------------------/\"\n",
    "#Tokenization\n",
    "def token_words(txt):\n",
    "    import nltk\n",
    "    return nltk.word_tokenize(txt)\n",
    "\n",
    "#Remove stop words\n",
    "def Token_and_removestopword(txt):\n",
    "    import nltk\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = nltk.word_tokenize(txt)\n",
    "    without_stop_words = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            without_stop_words.append(word)\n",
    "    return without_stop_words\n",
    "\n",
    "#Lemmatization \n",
    "def lemmatize_word(tokens,pos=\"v\"): \n",
    "    import nltk\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # provide context i.e. part-of-speech \n",
    "    lemmas = [lemmatizer.lemmatize(word, pos =pos) for word in tokens] \n",
    "    return lemmas\n",
    "\n",
    "#Stemming\n",
    "def stem_words(tokens): \n",
    "    import nltk\n",
    "    from nltk.stem.porter import PorterStemmer \n",
    "    stemmer = PorterStemmer()  \n",
    "    stems = [stemmer.stem(word) for word in tokens] \n",
    "    return stems \n",
    "\n",
    "#Part of speech taggingÂ \n",
    "def pos_tagging(word_tokens):\n",
    "    import nltk\n",
    "    from nltk import pos_tag \n",
    "    pos_tag(word_tokens)\n",
    "    txt_gra = pos_tag(word_tokens)\n",
    "    #chunking function \n",
    "    # Input from Pos_tagging\n",
    "    text = []\n",
    "    grammer = []\n",
    "    for i in range(len(txt_gra)):\n",
    "        txt = txt_gra[i][0]\n",
    "        gram = txt_gra[i][1]\n",
    "        text.append(txt)\n",
    "        grammer.append(gram)\n",
    "    return  text, grammer \n",
    "\n",
    "     \n",
    "def correct_spellings(tokens):\n",
    "    from spellchecker import SpellChecker\n",
    "    spell = SpellChecker()\n",
    "    misspelled_words = spell.unknown(tokens)\n",
    "    corrected_text = []\n",
    "    for word in tokens:\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1-hot encoding model\n",
    "def Onehotencoding(doc_list):\n",
    "    from numpy import array\n",
    "    from numpy import argmax\n",
    "    import pandas as pd\n",
    "    def toktotxt(txt_int):\n",
    "        if isinstance(txt_int[0], list):\n",
    "            text = txt_int.apply(lambda x :\" \".join(str(i) for i in x))\n",
    "        else:\n",
    "            text = txt_int\n",
    "        return text\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.preprocessing import OneHotEncoder\n",
    "    values = array(toktotxt(doc_list))\n",
    "    label_encoder = LabelEncoder()\n",
    "    integer_encoded = label_encoder.fit_transform(values)\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)    \n",
    "    def unique(lst): \n",
    "        x = np.array(lst) \n",
    "        return np.unique(x) \n",
    "    col = unique(values)\n",
    "    df = pd.DataFrame(onehot_encoded,index=values,columns=col)\n",
    "    return onehot_encoded\n",
    "\n",
    "#Bag-of-words model (Bow) [Countvectorizer] & N-grams language model\n",
    "def countvectorizer(train_int,test_int=None,Ngram_min=1,Ngram_max=1):\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    def toktotxt(txt_int):\n",
    "        if isinstance(txt_int[0], list):\n",
    "            text = txt_int.apply(lambda x :\" \".join(str(i) for i in x))\n",
    "        else:\n",
    "            text = txt_int\n",
    "        return text\n",
    "    train_txt = toktotxt(train_int)  \n",
    "    vectorizer = CountVectorizer(ngram_range = (Ngram_min,Ngram_max))\n",
    "    vectorizer.fit(train_txt)\n",
    "    X = vectorizer.transform(train_txt)\n",
    "    train = X.toarray()\n",
    "    if test_int is None:\n",
    "        out = train\n",
    "    else:\n",
    "        test_txt = toktotxt(test_int)\n",
    "        Y = vectorizer.transform(test_txt)\n",
    "        test = Y.toarray()\n",
    "        out = train, test\n",
    "    return out\n",
    "\n",
    "#TF-IDF\n",
    "def TFIDF(train_int,test_int=None,Ngram_min=1,Ngram_max=1):\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    def toktotxt(txt_int):\n",
    "        if isinstance(txt_int[0], list):\n",
    "            text = txt_int.apply(lambda x :\" \".join(str(i) for i in x))\n",
    "        else:\n",
    "            text = txt_int\n",
    "        return text\n",
    "    train_txt = toktotxt(train_int)  \n",
    "    vectorizer = TfidfVectorizer(ngram_range = (Ngram_min,Ngram_max))\n",
    "    vectorizer.fit(train_txt)\n",
    "    X = vectorizer.transform(train_txt)\n",
    "    train = X.toarray()\n",
    "    if test_int is None:\n",
    "        out = train\n",
    "    else:\n",
    "        test_txt = toktotxt(test_int)\n",
    "        Y = vectorizer.transform(test_txt)\n",
    "        test = Y.toarray()\n",
    "        out = train, test\n",
    "    return out\n",
    "\n",
    "#Word2vec language model\n",
    "def word2vector(txt_int,word,min_count=1):\n",
    "    def texttotok(txt_int):\n",
    "        import nltk\n",
    "        if isinstance(txt_int[0], list):\n",
    "            tokens = txt_int\n",
    "        else:\n",
    "            tokens = nltk.word_tokenize(txt_int)\n",
    "        return tokens\n",
    "    tokens = []\n",
    "    for i in txt_int:\n",
    "        tok = texttotok(i)\n",
    "        tokens.append(tok)\n",
    "    #print(tokens)\n",
    "    from gensim.models import Word2Vec\n",
    "    model = Word2Vec(tokens,min_count=min_count)\n",
    "    index_lst = list(model.wv.vocab)\n",
    "    ary = model[word]\n",
    "    return ary\n",
    "\n",
    "#Hash Vectorizer\n",
    "def Hashvect(train_int,test_int=None,Ngram_min=1,Ngram_max=1):\n",
    "    import pandas as pd\n",
    "    from sklearn.feature_extraction.text import HashingVectorizer\n",
    "    def toktotxt(txt_int):\n",
    "        if isinstance(txt_int[0], list):\n",
    "            text = txt_int.apply(lambda x :\" \".join(str(i) for i in x))\n",
    "        else:\n",
    "            text = txt_int\n",
    "        return text\n",
    "    train_txt = toktotxt(train_int)  \n",
    "    vectorizer = HashingVectorizer(ngram_range = (Ngram_min,Ngram_max))\n",
    "    vectorizer.fit(train_txt)\n",
    "    X = vectorizer.transform(train_txt)\n",
    "    train = X.toarray()\n",
    "    if test_int is None:\n",
    "        out = train\n",
    "    else:\n",
    "        test_txt = toktotxt(test_int)\n",
    "        Y = vectorizer.transform(test_txt)\n",
    "        test = Y.toarray()\n",
    "        out = train, test\n",
    "    return out\n",
    "\n",
    "#GloVe language model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensional Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Component Analysis\n",
    "#Principal Component Analysis (PCA)\n",
    "def PCA(X_train,Y_train=None,X_test=None,n=10):\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=n)\n",
    "    X = pca.fit(X_train,y_train)\n",
    "    train = pca.transform(X_train)\n",
    "    if X_test is None:\n",
    "        out = train\n",
    "    else:\n",
    "        test = pca.transform(X_test)\n",
    "        out = train, test\n",
    "    return out\n",
    "#Independent Component Analysis (ICA)\n",
    "def ICA(X_train,Y_train=None,X_test=None,n=100):\n",
    "    from sklearn.decomposition import FastICA\n",
    "    mod = FastICA(n_components=n)\n",
    "    X = mod.fit(X_train,y_train)\n",
    "    train = mod.transform(X_train)\n",
    "    if X_test is None:\n",
    "        out = train\n",
    "    else:\n",
    "        test = pca.transform(X_test)\n",
    "        out = train, test\n",
    "    return out\n",
    "#Linear Discriminant Analysis (LDA)\n",
    "def LDA(X_train,Y_train=None,X_test=None,n=100):\n",
    "    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "    LDA = LinearDiscriminantAnalysis(n_components=n)\n",
    "    X = LDA.fit(X_train,y_train)\n",
    "    test = LDA.transform(X_train)\n",
    "    if X_test is None:\n",
    "        out = train\n",
    "    else:\n",
    "        test = pca.transform(X_test)\n",
    "        out = train, test\n",
    "    return out\n",
    "\n",
    "#Non-Negative Matrix Factorization (NMF)\n",
    "def NMF(X_train,Y_train=None,X_test=None,n=100,init='random',random_state=0):\n",
    "    from sklearn.decomposition import NMF\n",
    "    mod = NMF(n_components=n, init=init, random_state=random_state)\n",
    "    X = mod.fit(X_train,y_train)\n",
    "    test = mod.transform(X_train)\n",
    "    if X_test is None:\n",
    "        out = train\n",
    "    else:\n",
    "        test = pca.transform(X_test)\n",
    "        out = train, test\n",
    "    return out\n",
    "\n",
    "#Gaussian Random Projection\n",
    "def gaurandpro(X_train,y_train=None,X_test=None):\n",
    "    from sklearn import random_projection\n",
    "    mod = random_projection.GaussianRandomProjection()\n",
    "    X = mod.fit(X_train,y_train)\n",
    "    test = mod.transform(X_train)\n",
    "    if X_test is None:\n",
    "        out = train\n",
    "    else:\n",
    "        test = pca.transform(X_test)\n",
    "        out = train, test\n",
    "    return out\n",
    "\n",
    "#Sparse random projection\n",
    "def sparandpro(X_train,y_train=None,X_test=None):\n",
    "    from sklearn import random_projection\n",
    "    mod = random_projection.SparseRandomProjection()\n",
    "    X = mod.fit(X_train,y_train)\n",
    "    test = mod.transform(X_train)\n",
    "    if X_test is None:\n",
    "        out = train\n",
    "    else:\n",
    "        test = pca.transform(X_test)\n",
    "        out = train, test\n",
    "    return out\n",
    "\n",
    "#Johnson Lindenstrauss Lemma\n",
    "def JLL(X_train,y_train=None,X_test=None,n=100,init='random'):\n",
    "    from sklearn.random_projection import johnson_lindenstrauss_min_dim\n",
    "    mod = NMF(n_components=n, init=init, random_state=0)\n",
    "    X = mod.fit(X_train,y_train)\n",
    "    test = mod.transform(X_train)\n",
    "    if X_test is None:\n",
    "        out = train\n",
    "    else:\n",
    "        test = pca.transform(X_test)\n",
    "        out = train, test\n",
    "    return out\n",
    "\n",
    "#T- distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "def TSNE(X_train,y_train=None,X_test=None,n=100):\n",
    "    from sklearn.manifold import TSNE\n",
    "    mod = TSNE(n_components=n)\n",
    "    X = mod.fit(X_train,y_train)\n",
    "    test = mod.transform(X_train)\n",
    "    if X_test is None:\n",
    "        out = train\n",
    "    else:\n",
    "        test = pca.transform(X_test)\n",
    "        out = train, test\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rocchio Classification\n",
    "def rocchio_clas(X_train, y_train,X_test):\n",
    "    from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "    model = NearestCentroid()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return preds\n",
    "#Boosting\n",
    "def boosting(X_train, y_train,X_test,n=20):\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    model = GradientBoostingClassifier(n_estimators=n)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return preds\n",
    "#Bagging\n",
    "def Bagging(X_train, y_train,X_test,n=20):\n",
    "    from sklearn.ensemble import BaggingClassifier\n",
    "    model = BaggingClassifier(n_estimators=n)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return preds\n",
    "#Logistic Regression\n",
    "def logreg(X_train, y_train,X_test,n=20):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return preds\n",
    "#Naive Bayes Text Classification\n",
    "#MultinomialNB\n",
    "def naivebiase_multinomial(X_train, y_train,X_test,n=20):\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return preds\n",
    "\n",
    "#Gaussian\n",
    "def naivebiase_gaussian(X_train, y_train,X_test,n=20):\n",
    "    from sklearn.naive_bayes import GaussianNB\n",
    "    model = GaussianNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return preds\n",
    "\n",
    "#Bernoullies\n",
    "def naivebiase_gaussian(X_train, y_train,X_test,n=20):\n",
    "    from sklearn.naive_bayes import BernoulliNB\n",
    "    model = BernoulliNB()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return preds\n",
    "\n",
    "#K-Nearest Neighbors Algorithm (KNN)\n",
    "def KNeighbors(X_train, y_train,X_test,n=2):\n",
    "    from sklearn.neighbors import KNeighborsClassifier\n",
    "    model = KNeighborsClassifier(n)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return preds\n",
    "#Support Vector Machine (SVM)\n",
    "def SVM(X_train, y_train,X_test):\n",
    "    from sklearn.svm import LinearSVC\n",
    "    model = LinearSVC()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return preds\n",
    "#Decision Tree\n",
    "def Decisiontree(X_train, y_train,X_test):\n",
    "    from sklearn import tree\n",
    "    model = tree.DecisionTreeClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return preds\n",
    "#Random Forests\n",
    "def Randomforest(X_train, y_train,X_test):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return preds\n",
    "\n",
    "#Different input formate for crf\n",
    "#Conditional Random Field (CRF)\n",
    "def crf(X_train, y_train,X_test,alg='lbfgs'):\n",
    "    import sklearn_crfsuite\n",
    "    model = sklearn_crfsuite.CRF(algorithm=alg)\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_test)\n",
    "    return preds\n",
    "\n",
    "#Deep Neural Networks (DNN)\n",
    "#Recurrent Neural Network (RNN)\n",
    "#Convolutional Neural Networks (CNN)\n",
    "#Deep Belief Network (DBN)\n",
    "#Hierarchical Attention Networks (HAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matthew correlation coefficient (MCC)\n",
    "def MCC(y_true, y_pred):\n",
    "    from sklearn.metrics import matthews_corrcoef\n",
    "    return matthews_corrcoef(y_true, y_pred)\n",
    "\n",
    "#Recall\n",
    "def recall(y_true, y_pred):\n",
    "    from sklearn.metrics import recall_score\n",
    "    return recall_score(y_true, y_pred)\n",
    "\n",
    "#Precision\n",
    "def precision(y_true, y_pred):\n",
    "    from sklearn.metrics import precision_score\n",
    "    return precision_score(y_true, y_pred)\n",
    "\n",
    "#Accuracy\n",
    "def accuracy(y_true, y_pred):\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    return accuracy_score(y_true, y_pred)\n",
    "\n",
    "#F Score\n",
    "def f_score(y_true, y_pred):\n",
    "    from sklearn.metrics import f1_score\n",
    "    return f1_score(y_true, y_pred)\n",
    "\n",
    "#Receiver Operating Characteristics (ROC)\n",
    "def ROC(y_true, y_pred):\n",
    "    from sklearn.metrics import roc_curve\n",
    "    return roc_curve(y_true, y_pred)\n",
    "    \n",
    "#Area Under ROC Curve (AUC)\n",
    "def AUC(y_true, y_pred):\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    return roc_auc_score(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
